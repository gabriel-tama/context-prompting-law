{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"  # specify which GPU(s) to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, LLMPredictor, ServiceContext, GPTVectorStoreIndex\n",
    "from llama_index.response.pprint_utils import pprint_response\n",
    "# from \n",
    "from langchain.embeddings import HuggingFaceEmbeddings,HuggingFaceInstructEmbeddings\n",
    "\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "import torch\n",
    "\n",
    "# device = 'cuda:2,3' if torch.cuda.is_available() else 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-20523239/.conda/envs/yolov8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-31 12:56:00.122074: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-31 12:56:02.033734: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-31 12:56:02.033855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-31 12:56:02.033867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/wizardLM-13B-1.0-fp16\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"TheBloke/wizardLM-13B-1.0-fp16\",load_in_8bit=True,trust_remote_code=True, device_map=device,)\n",
    "# embeddings = HuggingFaceInstructEmbeddings(\n",
    "#     model_name=\"hkunlp/instructor-large\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# # class ModelLLM(LLM):\n",
    "# generation_pipeline = pipeline(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "# #     return_full_text=True,\n",
    "#     task=\"text-generation\",\n",
    "# #     max_length=2048,\n",
    "#     max_new_tokens=100,\n",
    "#     temperature=0,\n",
    "#     top_p = .95,\n",
    "# #     stopping_criteria=stopping_criteria,\n",
    "# #     generation_config=generation_config,\n",
    "#     model_kwargs={\n",
    "#                     \"device_map\": \"auto\", \n",
    "#                     \"max_length\": 512, \n",
    "#                     \"temperature\": 0.01,\n",
    "#                     \"torch_dtype\":torch.bfloat16,\n",
    "#                     }\n",
    "# )\n",
    " \n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # setup prompts - specific to Camel\n",
    "# # # # from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "# # # # question_t5_template = \"\"\"\n",
    "# # # #     context: {context}\n",
    "# # # #     question: {question}\n",
    "# # # #     answer: \n",
    "# # # #     \"\"\"\n",
    "# # # # # This will wrap the default prompts that are internal to llama-index\n",
    "# # # # # taken from https://huggingface.co/Writer/camel-5b-hf\n",
    "# # # # query_wrapper_prompt = SimpleInputPrompt(\n",
    "# # # #     question_t5_template\n",
    "# # # # )\n",
    "# # # from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "\n",
    "# # # system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "# # # - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "# # # - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "# # # - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "# # # - StableLM will refuse to participate in anything that could harm a human.\n",
    "# # # \"\"\"\n",
    "\n",
    "# # # # This will wrap the default prompts that are internal to llama-index\n",
    "# # # query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "# # from langchain.prompts.chat import (\n",
    "# #     ChatPromptTemplate,\n",
    "# #     HumanMessagePromptTemplate,\n",
    "# #     SystemMessagePromptTemplate,\n",
    "# # )\n",
    "# # from llama_index.prompts import Prompt\n",
    "\n",
    "# # chat_text_qa_msgs = [\n",
    "# #     SystemMessagePromptTemplate.from_template(\n",
    "# #         \"Always answer the question, even if the context isn't helpful.\"\n",
    "# #     ),\n",
    "# #     HumanMessagePromptTemplate.from_template(\n",
    "# #         \"Context information is below.\\n\"\n",
    "# #         \"---------------------\\n\"\n",
    "# #         \"{context_str}\\n\"\n",
    "# #         \"---------------------\\n\"\n",
    "# #         \"Given the context information and not prior knowledge, \"\n",
    "# #         \"answer the question: {query_str}\\n\"\n",
    "# #     ),\n",
    "# # ]\n",
    "# # chat_text_qa_msgs_lc = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
    "# # text_qa_template = Prompt.from_langchain_prompt(chat_text_qa_msgs_lc)\n",
    "\n",
    "# # # Refine Prompt\n",
    "# # chat_refine_msgs = [\n",
    "# #     SystemMessagePromptTemplate.from_template(\n",
    "# #         \"Always answer the question, even if the context isn't helpful.\"\n",
    "# #     ),\n",
    "# #     HumanMessagePromptTemplate.from_template(\n",
    "# #         \"We have the opportunity to refine the original answer \"\n",
    "# #         \"(only if needed) with some more context below.\\n\"\n",
    "# #         \"------------\\n\"\n",
    "# #         \"{context_msg}\\n\"\n",
    "# #         \"------------\\n\"\n",
    "# #         \"Given the new context, refine the original answer to better \"\n",
    "# #         \"answer the question: {query_str}. \"\n",
    "# #         \"If the context isn't useful, output the original answer again.\\n\"\n",
    "# #         \"Original Answer: {existing_answer}\"\n",
    "# #     ),\n",
    "# # ]\n",
    "\n",
    "\n",
    "# # chat_refine_msgs_lc = ChatPromptTemplate.from_messages(chat_refine_msgs)\n",
    "# # refine_template = Prompt.from_langchain_prompt(chat_refine_msgs_lc)\n",
    "# from llama_index import Prompt\n",
    "\n",
    "# template = (\n",
    "# #     \"We have provided context information below. \\n\"\n",
    "# #     \"---------------------\\n\"\n",
    "# #     \"{{context_str}}\"\n",
    "# #     \"\\n---------------------\\n\"\n",
    "#     \"Given this information, please answer the question: {query_str}\\n\"\n",
    "# )\n",
    "# qa_template = Prompt(template)\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "\n",
    "# query_wrapper_prompt = SimpleInputPrompt(\n",
    "#     \"Below is an instruction that describes a task. \"\n",
    "#     \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "#     \"### Context:\\n{{context_str}}\\n\\n\"\n",
    "#     \"### Instruction:\\n{{query_str}}\\n\\n### Response:\"\n",
    "# )\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># Custom LM (Alpha version)\n",
    "- StableLM is a helpful law language model.\n",
    "- StableLM is more than just an information source, it can compare legal documents based on user context.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:51<00:00, 17.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import HuggingFaceLLM\n",
    "\n",
    "# NOTE: the first run of this will download/cache the weights, ~20GB\n",
    "hf_predictor = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0, \"do_sample\": False},\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"TheBloke/wizardLM-13B-1.0-fp16\",\n",
    "#     model_name=\"lmsys/vicuna-13b-v1.3\",\n",
    "    model_name=\"TheBloke/wizardLM-13B-1.0-fp16\",\n",
    "    device_map='auto',\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    tokenizer_outputs_to_remove=[\"token_type_ids\"],\n",
    "\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "class InstructorEmbeddings(BaseEmbedding):\n",
    "    def __init__(\n",
    "    self, \n",
    "    instructor_model_name: str = \"hkunlp/instructor-large\",\n",
    "        instruction: str = \"Represent the document for pasal retrieval\",\n",
    "    **kwargs: Any,) -> None:\n",
    "        self._model = INSTRUCTOR(instructor_model_name)\n",
    "        self._instruction = instruction\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        embeddings = self._model.encode([[self._instruction, query]])\n",
    "        return embeddings[0]\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        embeddings = self._model.encode([[self._instruction, text]])\n",
    "        return embeddings[0] \n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = self._model.encode([[self._instruction, text] for text in texts])\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    LangchainEmbedding,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    download_loader,\n",
    "    PromptHelper,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "# embed_model = LangchainEmbedding(InstructorEmbeddings())\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=512, llm=hf_predictor, embed_model=InstructorEmbeddings(embed_batch_size=2))\n",
    "# service_context = ServiceContext.from_defaults(embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc_1 = SimpleDirectoryReader(input_files=['../Investasi/Peraturan_Presiden_No.10_Tahun_1964.pdf']).load_data()\n",
    "# print(len(doc_1))\n",
    "len(doc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_2 = SimpleDirectoryReader(input_files=['../Investasi/Perpu Nomor 1 Tahun 2020.pdf']).load_data()\n",
    "len(doc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index import ListIndex\n",
    "doc_1_idx = ListIndex.from_documents(doc_1,service_context=service_context)\n",
    "# print(len)\n",
    "len(doc_1_idx.docstore.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ListIndex\n",
    "doc_2_idx = ListIndex.from_documents(doc_2,service_context=service_context)\n",
    "# print(len)\n",
    "# len(doc_1_idx.docstore.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1_idx.set_index_id(\"vector_index\")\n",
    "doc_1_idx.storage_context.persist(\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild storage context\n",
    "from llama_index import     load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, index_id=\"vector_index\",service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = doc_1_idx.as_query_engine(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Is there any direct contradiction in the document?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the additional context provided, there does not appear to be any direct contradiction in the document. The document establishes a committee for the management of state funds for development projects and outlines its responsibilities, including providing recommendations on budgeting, resource allocation, and investment strategies. The document also mentions that the committee will be supported by teams and that its expenses will be covered by the National Planning Development Budget. The document also states that any matters not covered in the decree will be regulated by further regulations issued by the Minister of National Planning. This statement does not contradict any other statements in the document and provides clarity on how any unaddressed matters will be handled. Therefore, the answer to the original question remains the same: Based on the given context information, there does not appear to be any direct contradiction in the document.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the new context provided, there is no direct contradiction in the document. The document discusses the tariff for the central investment agency's central agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment agency's investment\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_2_idx = ListIndex.from_documents(doc_2,service_context=service_context)\n",
    "# print(len)\n",
    "len(doc_2_idx.docstore.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1_engine = doc_1_idx.as_query_engine(service_context=service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_2_engine = doc_2_idx.as_query_engine(service_context=service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools=[\n",
    "    QueryEngineTool(\n",
    "        query_engine=doc_1_engine,\n",
    "        metadata=ToolMetadata(name='Peraturan Menteri Keuangan No.5 2021',description='Contain Set of Rules/Regulation or Pasal about public service agency rates')\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=doc_2_engine,\n",
    "        metadata=ToolMetadata(name='Perpu Nomor 1 Tahun 2020',description='Contain set of rules/regulation or pasal about monetary policy regarding covid-19'))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.query_engine import RouterQueryEngine\n",
    "\n",
    "# query_engine = RouterQueryEngine.from_defaults(\n",
    "#     query_engine_tools=query_engine_tools\n",
    "# )\n",
    "\n",
    "# response = query_engine.query(\n",
    "#     \"In Notion, give me a summary of the product roadmap.\"\n",
    "# )\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools,service_context=service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = doc_1_idx.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Is there any contradiction ? translate the answer in Indonesian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legalenv",
   "language": "python",
   "name": "legalenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
